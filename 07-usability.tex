% Objective 2 : Assess the usability, usefulness, and technical barriers to a gamified \gls{eaf} intervention for physicians at the MUHC.
%
%
The field of \gls{EAF} is emerging and evidence is needed to guide design decisions to maximize usability \cite{brown2016interface}. There is no evidence regarding how design decisions about behavioral factors such as gamification affect the usability of \gls{EAF}. Furthermore, preliminary validation of a gamified system is needed to justify spending the time and resources required for a randomized trial.

Consequently, I aim to assess the usability and usefulness of a gamified \gls{EAF} intervention for physicians at the \gls{MUHC}. A secondary aim will be to identify potential technical difficulties in deploying the system for the \gls{RCT}. This assessment will ensure that the system is usable and is free from obvious defects. The assement  also presents an opportunity to verify that the metrics needed for objective 3 can be automatically collected and to generate pilot data to inform the design of the RCT.

\section{Research Design}
I will perform laboratory-based usability testing of the gamified intervention using a convenience sample of eligible physicians starting with 5 and adding 3 more until saturation is reached. Saturation occurs when adding more participants does not result in additional perspectives \cite{green2018qualitative}. During this evaluation the participants will be asked to accomplish the same set of tasks, using simulated but realistic data. The tasks will test core \gls{EAF} functionalities, using goals from the evaluation of the control system, as well as gamified elements. Usability scores of the gamified system will not be compared to the control, due to their use of a different test methodology and different set of functionalities.

The sample size is influenced by the homogeneity of the users (all physicians with high education), and prior evidence on the relation between sample size and the number of usability problems uncovered \cite{nielsen1993mathematical}. Since the usability of the control was already explored by its authors, only the experimental system will be assessed \cite{brown2016interface}. Usability issues uncovered by this exercise will be used to orient pre-trial software development efforts. This testing is summative, meaning it seeks to examine and evaluate how effectively concepts have been implemented \cite{rubin2008handbook}.

I define usability as ``the extent to which a system, product or service can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use'' \cite{international1998iso} As opposed to usefulness, or the ability of a system to ``provide content that is engaging, relevant, and appropriate to the audience''. Both constructs are often used in user experience design and have strong face and content validity \cite{united2006research}. The secondary aim of identifying technical difficulties will be assessed qualitatively from the experience of the research team in setting up the test. It will serve to adapt the solution, to plan for trial deployment.

\section{Procedure}
Participants will first be required to provide informed consent and then complete a pre-test questionnaire to measure age, gender, specialty, number of year of clinical practice, and familiarity with information technology. Then participants will be presented with a series of tasks to accomplish while ``thinking aloud''. The think aloud procedure provides greater insight into the user's mental model by asking them to verbalize their thoughts. Think aloud sessions can elicit a larger set of usability issues than observation alone \cite{ericsson1980verbal}. A researcher will be present to record the thought process and note any difficulties or confusion as the user performs the tasks. Successful completion will be assessed on a scale of 0 to 4, with 0 meaning the user could only accomplish the task with substantial guidance, and 4 meaning the user solved the tasks without help \cite{rubin2008handbook}. The researcher will only provide guidance if the user seeks help or abandons the task. The provided tasks will be centred on the four main features of \gls{EAF} (described in the next section) along with additional tasks to cover the gamified material. I will create these tasks once the intervention is designed and they will be discussed with my thesis committee.

Upon completing the tasks, participants will be asked to complete a TAM2 (technology acceptance model v2) questionnaire and provide any textual unstructured feedback they might want to share at the same time. The TAM2 questionnaire is a validated instrument for measuring perceived usefulness and perceived ease of use, which are important determinants of the attitudes and intentions related with use behavior \cite{venkatesh2000theoretical}. The TAM2 will collect data on projected adoption, and I will also ask participants to estimate their frequency of use of the inervention in scenarios of low, medium, and high baseline performance and with low and high rates of HF patients.
% More info on TAM2 Based on Theory of Reasoned Action Sub. Cri. : Computer Anxiety, Comp. Playfulness, Comp. Self-Efficacy, external control.

\section{A\&F Systems}
Both the experimental and control versions will use electronic audit and feedback systems following the four key components of \gls{EAF} system interfaces: (1) Summaries of clinical performance; (2) Patient lists; (3) Patient-level data; and (4) Recommended actions \cite{brown2015meta}. The control system will be based on the user experience and interfaces described in the PINGR system \cite{brown2016interface}, but adapted for the measures identified in objective 1 and for the collection of data needed in objective 3. I have contact the creator of PINGR, who has offered to help if I have any questions. The experimental system will be built by the McGill Clinical and Health Informatics (MCHI) software development team. They have many years of experience in system development, in usability testing, and in human centered design \cite{shaban2017pophr, lavigne2013hybrid, buckeridge2016developing}.

The design of the experimental system will be largely driven by 1) the ``GOAL framework'' for gamification in software engineering \cite{garcia2017framework}, and 2) the ``Rules of Play'', which is  the authoritative textbook on game design \cite{salen2004rules}. There are four major category of possible game elements; game dynamics (overarching design), game mechanics (step by step processes), specific components (tangible elements of dynamics and mechanics, and aesthetics (design producing positive affect) \cite{mckeown2016gamification}. In order to maintain alignment with the research aims and user-specific concerns when designing the system, recommendations from gamified interventions in similar settings and with similar users will be followed \cite{mckeown2016gamification}. A scrum based software development methodology will be used for its flexibility with local evolving needs, its use of short fast-paced development phases, and its use of an embedded product owner, which in this case will be a local expert \cite{cohn2010succeeding}. This software development methodology enables the use of an agile model of software development \cite{beck2001manifesto}.

\section{Analysis Plan}
The results of this analysis will be threefold: a per task success score, a thematically organised set of issues and feedback, and a TAM2 based compound usability and usefulness score. Special attention will be taken to elicit possible misunderstandings, or adverse effects. These results will be analyzed to assess the potential influence of demographic factors and how they relate to specific interface elements.

% 1 - The results won't be compared because you used different methodology? Why didn't you use the same then?
% 2 - What if the user fails even after strong guidance.
%   - We assume it's still 0-strong guidance.
% 3 - Does the task scenario need to be validated?
%   - I would say not, other than my committee. 
% => How is mock data generated?
% => Real data will elicit more relevant comment and might be the only way to get participant to dig deeply in what is being presented. However, fake data as the advantage of not being so sensitive. Should we choose individuals that would be participants and show them their own data?
% => Contamination of the usability participants?

% Additional References
% => [Usability Website](https://www.usabilitybok.org/usability-testing)

% Cut out

% \textbf{Technical Feasibility} Will this be hosted in-hospital, would they have access outside the hospital. Mobile or Browser based. If inside the hospital, are there computer there able to connect/view the page. If yes, would they have/be given time to view the page and explore it. +(Use the same network and IT resources, maybe connect to the same kind of portal? To assess if it scales.)
% the size of the eligible  population,